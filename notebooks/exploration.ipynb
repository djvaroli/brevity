{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import tiktoken\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://arxiv.org/abs/2301.12811\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = requests.get(url).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = tokenizer.encode(content)\n",
    "tok_partition = all_tokens[:2000]\n",
    "content_partition = tokenizer.decode(tok_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You will be presented with HTML containing information fetched from a URL. \n",
    "Suggest a title for the content of the page and generate a 3 - 4 sentence summary of the content.\n",
    "\n",
    "Return a JSON object with the following fields: \"title\" and \"summary\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": content_partition}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = [\n",
    "  \"This paper addresses the question of whether the optimization process in generative adversarial networks (GANs) actually provides the generator with gradients that make its distribution close to the target distribution. The authors derive metrizable conditions, which are sufficient conditions for the discriminator to serve as the distance between the distributions by connecting the GAN formulation with the concept of sliced optimal transport. They propose a novel GAN training scheme called slicing adversarial network (SAN), which can be applied to a broad class of existing GANs with simple modifications. Experimental results support the effectiveness of SAN compared to usual GANs.\",\n",
    "  \"This paper presents a method called SAN (Self-Attention-based Normalization) to induce metrizability of Generative Adversarial Networks (GANs) using a discriminative normalized linear layer. The proposed method aims to improve the sample quality and diversity of GANs by addressing the mode collapse and sensitivity to input distribution issues. Experimental results show that SAN achieves better performance compared to baseline methods on various benchmark datasets. The authors also provide a theoretical analysis of the proposed method and demonstrate its effectiveness through visualizations and qualitative evaluations.\",\n",
    "  \"This paper explores the optimization process of generative adversarial networks (GANs) and investigates whether it actually provides the generator with gradients that make its distribution close to the target distribution. The authors derive metrizable conditions for the discriminator to serve as the distance between the distributions, connecting the GAN formulation with the concept of sliced optimal transport. They propose a new GAN training scheme called slicing adversarial network (SAN) which can convert a broad class of existing GANs into SANs. Experimental results show that SAN is effective and can achieve state-of-the-art performance in class conditional generation.\",\n",
    "  \"This page contains information about bibliographic and citation tools. The page lists various tools such as the Bibliographic Explorer, Litmaps, and scite Smart Citations. Each tool has a toggle switch that allows the user to enable or disable it. The page also includes links to other code, data, and media associated with the article.\",\n",
    "  \"The HTML contains toggle switches for various tools and demos. The tools include DagsHub, Papers with Code, and ScienceCast, while the demos include Replicate and Hugging Face Spaces. The page also has a section for related papers, featuring tools like Influence Flower, Connected Papers, CORE Recommender, and IArxiv Recommender.\",\n",
    "  \"arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. If you have an idea for a project that will add value for arXiv's community, you can learn more about arXivLabs on our website.\",\n",
    "  \"This HTML contains contact and subscription information for arXiv. It includes links to pages about arXiv, help, contact information, and how to subscribe to arXiv mailings. It also provides links to pages about copyright, privacy policy, and web accessibility assistance.\",\n",
    "  \"This HTML code snippet represents the footer section of a website. It contains links to several social media platforms including Facebook, Twitter, and Slack. The footer also includes a JavaScript script tag that references an external script file.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'SAN: Inducing Metrizability of GAN with Discriminative Normalized Linear Layer',\n",
       " 'summary': 'Generative adversarial networks (GANs) learn a target probability distribution by optimizing a generator and a discriminator with minimax objectives. This paper addresses the question of whether such optimization actually provides the generator with gradients that make its distribution close to the target distribution. We derive metrizable conditions, sufficient conditions for the discriminator to serve as the distance between the distributions by connecting the GAN formulation with the concept of sliced optimal transport. Furthermore, by leveraging these theoretical results, we propose a novel GAN training scheme, called slicing adversarial network (SAN).'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt = \"\"\"\n",
    "You will be presented with several summaries and corresponding titles, each of which is obtained by condensing the HTML from a web page.\n",
    "Each summary represents a piece of the same larger HTML document.\n",
    "\n",
    "Your job is to merge the list of summaries into a single, semantically consistent and correct summary and title.\n",
    "Provide clear explanations for your reasoning and how much weight you gave to each summary. Prioritize summaries that\n",
    "are more likely to be relevant to the content a human user would be interested in.\n",
    "\n",
    "Then generate a 3 - 4 sentence summary combining the information from all of the summaries and give the summary a title.\n",
    "\n",
    "Return a JSON object with the following fields: \"title\" and \"summary\".\n",
    "\"\"\"\n",
    "text = \"\\n\\n\".join([f\"Title: {summary.title}\\nSummary: {summary.summary}\" for summary in summaries])\n",
    "\n",
    "summary_response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "url = \"https://arxiv.org/pdf/2309.10668.pdf\"\n",
    "\n",
    "# response = requests.get(url)\n",
    "\n",
    "# with open('paper.pdf', 'wb') as f:\n",
    "#     f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2309.10668', '.pdf', '2309.10668.pdf')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(url).stem, Path(url).suffix, Path(url).name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"paper.pdf\").unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textract\n",
    "text = textract.process('paper.pdf').decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokens = tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14853"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import requests\n",
    "import textract\n",
    "\n",
    "from app.file.rnd import gen_random_string\n",
    "\n",
    "class RemoteFile:\n",
    "    def __init__(\n",
    "        self,\n",
    "        url: str\n",
    "    ) -> None:\n",
    "        self.url = url\n",
    "        self._local_path = Path(f\"{gen_random_string()}{self.extension}\")\n",
    "        self._is_deleted = False\n",
    "        self._is_downloaded = False\n",
    "\n",
    "    def delete(self):\n",
    "        self.local_path.unlink()\n",
    "        self._is_deleted = True\n",
    "\n",
    "    def download(self, headers: Optional[Dict[str, str]] = None) -> None:\n",
    "        \"\"\"Download the file at the given URL.\n",
    "\n",
    "        Args:\n",
    "            headers (Optional[Dict[str, str]], optional): HTTP headers to send with the request. Defaults to None.\n",
    "        \"\"\"\n",
    "        # generate a random file name to avoid collisions\n",
    "        response = requests.get(self.url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        with open(self._local_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        self._is_downloaded = True\n",
    "    \n",
    "    @property\n",
    "    def _url_path(self) -> Path:\n",
    "        return Path(self.url)\n",
    "    \n",
    "    @property\n",
    "    def extension(self) -> str:\n",
    "        return self._url_path.suffix\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return self._url_path.name\n",
    "\n",
    "    @property\n",
    "    def stem(self) -> str:\n",
    "        return self._url_path.stem\n",
    "    \n",
    "    def extract_text(self) -> str:\n",
    "        \"\"\"Extract text from the file.\n",
    "\n",
    "        Returns:\n",
    "            str: text content of the file.\n",
    "        \"\"\"\n",
    "        return textract.process(str(self._local_path)).decode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_file = RemoteFile(\"https://example-files.online-convert.com/document/txt/example.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('.txt', 'example')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remote_file.extension, remote_file.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_file.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = remote_file.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121314"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(121314, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
