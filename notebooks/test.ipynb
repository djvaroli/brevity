{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielvaroli/opt/anaconda3/envs/arxiver/lib/python3.11/site-packages/pydantic/_internal/_fields.py:128: UserWarning: Field \"model_name\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "\n",
    "from app.prompts import (\n",
    "    SummarizationPrompt, \n",
    "    ChainOfDensityPrompt, \n",
    "    SummaryLength, \n",
    "    Summary,\n",
    "    JoinSummariesPrompt\n",
    ")\n",
    "from app.file.remote import RemoteFile\n",
    "from app.schema import SummaryParameters, SummaryResponse\n",
    "from app.summarizer import AITextSummarizer, AIModel\n",
    "from app.display import print_to_console\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChainOfDensityPrompt(SummaryLength.LONG)\n",
    "summarizer = AITextSummarizer(\n",
    "    AIModel.gpt_3_5_turbo,\n",
    "    prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"chunk-0.txt\", \"r\") as f:\n",
    "    chunk = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">┃                                                 OpenAI Response                                                 ┃</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34m┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\u001b[0m\n",
       "\u001b[1;34m┃\u001b[0m\u001b[1;34m \u001b[0m\u001b[1;34m                                                \u001b[0m\u001b[1;34mOpenAI Response\u001b[0m\u001b[1;34m                                                \u001b[0m\u001b[1;34m \u001b[0m\u001b[1;34m┃\u001b[0m\n",
       "\u001b[1;34m┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-87vzOzX6b1ZgzlVxh2fibd59oC72t\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1696901898,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"[{\\\"Missing_Entities\\\": \\\"predictive models, lossless compressors\\\", \\\"Denser_Summary\\\": \\\"This article discusses the connection between predictive models and lossless compressors. It highlights that large language models can be strong compressors due to their impressive predictive capabilities. The article advocates for viewing the prediction problem through the lens of compression and evaluates the compression capabilities of large language models. It shows that these models can compress ImageNet patches to 43.4% and LibriSpeech samples to 16.4% of their raw size, outperforming domain-specific compressors like PNG and FLAC. The article also explores the use of compressors as generative models and provides insights into scaling laws and tokenization.\\\"}, {\\\"Missing_Entities\\\": \\\"probabilistic models, source coding theorem\\\", \\\"Denser_Summary\\\": \\\"This article discusses the connection between probabilistic models and the source coding theorem. It explains that maximizing the log2-likelihood of a statistical model is equivalent to minimizing the number of bits required per message in lossless compression. The article explores different coding distributions and introduces arithmetic coding as an optimal method for compression. It highlights the importance of coding distributions in achieving compression performance and explains the concept of Shannon entropy. The article also discusses the connection between compression and prediction, emphasizing that compressing well means modeling well in a log-loss sense.\\\"}, {\\\"Missing_Entities\\\": \\\"offline compression, in-context learning, context length\\\", \\\"Denser_Summary\\\": \\\"This article focuses on offline compression and in-context learning. It explains that offline compression trains a model on an external dataset before using it to compress a data stream. The article highlights the key role of context length in offline compression, as it determines the maximum number of bytes a model can compress at a time. It discusses the limitations of Transformers in terms of context length and the need for extending these models' context lengths. The article also emphasizes the in-context learning abilities of Transformers, which make them well-suited for offline compression.\\\"}, {\\\"Missing_Entities\\\": \\\"scaling laws, dataset size, model size\\\", \\\"Denser_Summary\\\": \\\"This article explores scaling laws in the context of compression. It shows that dataset size acts as a hard limit on model size in terms of compression performance. It explains that scaling beyond a certain point can deteriorate compression performance because the model parameters need to be accounted for in the compressed output. The article highlights the importance of considering the trade-off between model size and compression performance when scaling up. It emphasizes that scaling is not a silver bullet and that careful consideration of the dataset size is necessary.\\\"}, {\\\"Missing_Entities\\\": \\\"tokenization, pre-compression, information content\\\", \\\"Denser_Summary\\\": \\\"This article discusses the role of tokenization in compression. It explains that tokenization can be viewed as a form of pre-compression but does not necessarily improve compression performance. Instead, tokenization allows models to increase the information content in their context, which improves prediction performance. The article highlights the general use of tokenization to enhance prediction capabilities in large language models. It emphasizes that tokenization is employed to increase the information content in the model's context, leading to improved prediction performance.\\\"}]\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 3347,\n",
      "    \"completion_tokens\": 648,\n",
      "    \"total_tokens\": 3995\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[{\"Missing_Entities\": \"predictive models, lossless compressors\", \"Denser_Summary\": \"This article discusses the     </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">connection between predictive models and lossless compressors. It highlights that large language models can be     </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">strong compressors due to their impressive predictive capabilities. The article advocates for viewing the          </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">prediction problem through the lens of compression and evaluates the compression capabilities of large language    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">models. It shows that these models can compress ImageNet patches to 43.4% and LibriSpeech samples to 16.4% of their</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">raw size, outperforming domain-specific compressors like PNG and FLAC. The article also explores the use of        </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">compressors as generative models and provides insights into scaling laws and tokenization.\"}, {\"Missing_Entities\": </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">\"probabilistic models, source coding theorem\", \"Denser_Summary\": \"This article discusses the connection between    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">probabilistic models and the source coding theorem. It explains that maximizing the log2-likelihood of a           </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">statistical model is equivalent to minimizing the number of bits required per message in lossless compression. The </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">article explores different coding distributions and introduces arithmetic coding as an optimal method for          </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">compression. It highlights the importance of coding distributions in achieving compression performance and explains</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">the concept of Shannon entropy. The article also discusses the connection between compression and prediction,      </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">emphasizing that compressing well means modeling well in a log-loss sense.\"}, {\"Missing_Entities\": \"offline        </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">compression, in-context learning, context length\", \"Denser_Summary\": \"This article focuses on offline compression  </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">and in-context learning. It explains that offline compression trains a model on an external dataset before using it</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">to compress a data stream. The article highlights the key role of context length in offline compression, as it     </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">determines the maximum number of bytes a model can compress at a time. It discusses the limitations of Transformers</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">in terms of context length and the need for extending these models' context lengths. The article also emphasizes   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">the in-context learning abilities of Transformers, which make them well-suited for offline compression.\"},         </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">{\"Missing_Entities\": \"scaling laws, dataset size, model size\", \"Denser_Summary\": \"This article explores scaling    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">laws in the context of compression. It shows that dataset size acts as a hard limit on model size in terms of      </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">compression performance. It explains that scaling beyond a certain point can deteriorate compression performance   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">because the model parameters need to be accounted for in the compressed output. The article highlights the         </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">importance of considering the trade-off between model size and compression performance when scaling up. It         </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">emphasizes that scaling is not a silver bullet and that careful consideration of the dataset size is necessary.\"}, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">{\"Missing_Entities\": \"tokenization, pre-compression, information content\", \"Denser_Summary\": \"This article         </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">discusses the role of tokenization in compression. It explains that tokenization can be viewed as a form of        </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">pre-compression but does not necessarily improve compression performance. Instead, tokenization allows models to   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">increase the information content in their context, which improves prediction performance. The article highlights   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">the general use of tokenization to enhance prediction capabilities in large language models. It emphasizes that    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">tokenization is employed to increase the information content in the model's context, leading to improved prediction</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">performance.\"}]                                                                                                    </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[{\"Missing_Entities\": \"predictive models, lossless compressors\", \"Denser_Summary\": \"This article discusses the     \u001b[0m\n",
       "\u001b[1;32mconnection between predictive models and lossless compressors. It highlights that large language models can be     \u001b[0m\n",
       "\u001b[1;32mstrong compressors due to their impressive predictive capabilities. The article advocates for viewing the          \u001b[0m\n",
       "\u001b[1;32mprediction problem through the lens of compression and evaluates the compression capabilities of large language    \u001b[0m\n",
       "\u001b[1;32mmodels. It shows that these models can compress ImageNet patches to 43.4% and LibriSpeech samples to 16.4% of their\u001b[0m\n",
       "\u001b[1;32mraw size, outperforming domain-specific compressors like PNG and FLAC. The article also explores the use of        \u001b[0m\n",
       "\u001b[1;32mcompressors as generative models and provides insights into scaling laws and tokenization.\"}, {\"Missing_Entities\": \u001b[0m\n",
       "\u001b[1;32m\"probabilistic models, source coding theorem\", \"Denser_Summary\": \"This article discusses the connection between    \u001b[0m\n",
       "\u001b[1;32mprobabilistic models and the source coding theorem. It explains that maximizing the log2-likelihood of a           \u001b[0m\n",
       "\u001b[1;32mstatistical model is equivalent to minimizing the number of bits required per message in lossless compression. The \u001b[0m\n",
       "\u001b[1;32marticle explores different coding distributions and introduces arithmetic coding as an optimal method for          \u001b[0m\n",
       "\u001b[1;32mcompression. It highlights the importance of coding distributions in achieving compression performance and explains\u001b[0m\n",
       "\u001b[1;32mthe concept of Shannon entropy. The article also discusses the connection between compression and prediction,      \u001b[0m\n",
       "\u001b[1;32memphasizing that compressing well means modeling well in a log-loss sense.\"}, {\"Missing_Entities\": \"offline        \u001b[0m\n",
       "\u001b[1;32mcompression, in-context learning, context length\", \"Denser_Summary\": \"This article focuses on offline compression  \u001b[0m\n",
       "\u001b[1;32mand in-context learning. It explains that offline compression trains a model on an external dataset before using it\u001b[0m\n",
       "\u001b[1;32mto compress a data stream. The article highlights the key role of context length in offline compression, as it     \u001b[0m\n",
       "\u001b[1;32mdetermines the maximum number of bytes a model can compress at a time. It discusses the limitations of Transformers\u001b[0m\n",
       "\u001b[1;32min terms of context length and the need for extending these models' context lengths. The article also emphasizes   \u001b[0m\n",
       "\u001b[1;32mthe in-context learning abilities of Transformers, which make them well-suited for offline compression.\"},         \u001b[0m\n",
       "\u001b[1;32m{\"Missing_Entities\": \"scaling laws, dataset size, model size\", \"Denser_Summary\": \"This article explores scaling    \u001b[0m\n",
       "\u001b[1;32mlaws in the context of compression. It shows that dataset size acts as a hard limit on model size in terms of      \u001b[0m\n",
       "\u001b[1;32mcompression performance. It explains that scaling beyond a certain point can deteriorate compression performance   \u001b[0m\n",
       "\u001b[1;32mbecause the model parameters need to be accounted for in the compressed output. The article highlights the         \u001b[0m\n",
       "\u001b[1;32mimportance of considering the trade-off between model size and compression performance when scaling up. It         \u001b[0m\n",
       "\u001b[1;32memphasizes that scaling is not a silver bullet and that careful consideration of the dataset size is necessary.\"}, \u001b[0m\n",
       "\u001b[1;32m{\"Missing_Entities\": \"tokenization, pre-compression, information content\", \"Denser_Summary\": \"This article         \u001b[0m\n",
       "\u001b[1;32mdiscusses the role of tokenization in compression. It explains that tokenization can be viewed as a form of        \u001b[0m\n",
       "\u001b[1;32mpre-compression but does not necessarily improve compression performance. Instead, tokenization allows models to   \u001b[0m\n",
       "\u001b[1;32mincrease the information content in their context, which improves prediction performance. The article highlights   \u001b[0m\n",
       "\u001b[1;32mthe general use of tokenization to enhance prediction capabilities in large language models. It emphasizes that    \u001b[0m\n",
       "\u001b[1;32mtokenization is employed to increase the information content in the model's context, leading to improved prediction\u001b[0m\n",
       "\u001b[1;32mperformance.\"}]                                                                                                    \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary = summarizer.summarize(chunk, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
